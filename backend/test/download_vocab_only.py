import json
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from typing import List, Dict
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class RAGEngine:
    def __init__(self, documents_path: str = "./documents.json"):
        print("ğŸš€ åˆå§‹åŒ– RAG å¼•æ“...")
        
        # åŠ è½½æ–‡æ¡£
        self.documents = self._load_documents(documents_path)
        print(f"ğŸ“š åŠ è½½äº† {len(self.documents)} ä¸ªæ–‡æ¡£")
        
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹
        print("ğŸ§  åŠ è½½å‘é‡æ¨¡å‹...")
        model_path = "./models/paraphrase-multilingual-MiniLM-L12-v2"
        if os.path.exists(model_path):
            print("ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„è€Œä¸æ˜¯ç»å¯¹è·¯å¾„ï¼Œé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
            self.model = SentenceTransformer(model_path)
        else:
            raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path}ã€‚è¯·ç¡®ä¿æ¨¡å‹å·²ä¸‹è½½åˆ°æŒ‡å®šä½ç½®ã€‚")
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # æ„å»ºå‘é‡ç´¢å¼•
        self._build_vector_index()
        
    def _load_documents(self, path: str) -> List[Dict]:
        """åŠ è½½æ–‡æ¡£æ•°æ®"""
        # å°è¯•å¤šä¸ªè·¯å¾„
        possible_paths = [
            path,
            './documents.json',
        ]
        
        for file_path in possible_paths:
            try:
                if os.path.exists(file_path):
                    print(f"âœ… æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶: {file_path}")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        documents = json.load(f)
                    print(f"ğŸ“„ æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
                    return documents
            except Exception as e:
                print(f"âš ï¸ å°è¯•è·¯å¾„ {file_path} å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ–‡æ¡£
        print("âŒ æœªæ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æ–‡æ¡£")
        return self._get_default_documents()
    
    def _get_default_documents(self) -> List[Dict]:
        """è·å–é»˜è®¤æ–‡æ¡£"""
        return [
            {
                "title": "äººå·¥æ™ºèƒ½ç®€ä»‹",
                "content": "äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯æŒ‡ç”±äººå·¥åˆ¶é€ å‡ºæ¥çš„ç³»ç»Ÿæ‰€è¡¨ç°å‡ºæ¥çš„æ™ºèƒ½ã€‚äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒé—®é¢˜åŒ…æ‹¬æ¨ç†ã€çŸ¥è¯†ã€è§„åˆ’ã€å­¦ä¹ ã€äº¤æµã€æ„ŸçŸ¥ã€ç§»åŠ¨å’Œæ“ä½œç‰©ä½“çš„èƒ½åŠ›ç­‰ã€‚AIæŠ€æœ¯å·²ç»å¹¿æ³›åº”ç”¨äºåŒ»ç–—è¯Šæ–­ã€é‡‘èé£æ§ã€è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸã€‚",
                "metadata": {"category": "technology", "source": "wikipedia"}
            },
            {
                "title": "æœºå™¨å­¦ä¹ åŸºç¡€",
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ— ç›‘ç£å­¦ä¹ å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ï¼Œå¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æœºåˆ¶å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚",
                "metadata": {"category": "technology", "source": "educational"}
            },
            {
                "title": "æ·±åº¦å­¦ä¹ å‘å±•",
                "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚è¿‘å¹´æ¥åœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ“…é•¿å¤„ç†å›¾åƒæ•°æ®ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œ(RNN)é€‚åˆåºåˆ—æ•°æ®å¤„ç†ï¼ŒTransformeræ¶æ„åˆ™é©æ–°äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸã€‚",
                "metadata": {"category": "technology", "source": "research"}
            }
        ]
    
    def _build_vector_index(self):
        """æ„å»ºå‘é‡ç´¢å¼•"""
        print("ğŸ—ï¸ æ„å»ºå‘é‡ç´¢å¼•...")
        
        # è·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹
        texts = [doc['content'] for doc in self.documents]
        
        # ç”Ÿæˆå‘é‡
        print("ğŸ”¢ ç”Ÿæˆæ–‡æ¡£å‘é‡...")
        embeddings = self.model.encode(texts)
        
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.index = faiss1.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        
        # å½’ä¸€åŒ–å‘é‡å¹¶æ·»åŠ åˆ°ç´¢å¼•
        faiss1.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        print("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
    
    def search(self, query: str, top_k: int = 5, threshold: float = 0.5) -> List[Dict]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: '{query}'")
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.model.encode([query])
        faiss1.normalize_L2(query_embedding)
        
        # æœç´¢
        similarities, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # å¤„ç†ç»“æœ
        results = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if similarity >= threshold and idx < len(self.documents):
                doc = self.documents[idx]
                results.append({
                    'id': int(idx),
                    'content': doc['content'],
                    'title': doc['title'],
                    'similarity': float(similarity),
                    'metadata': doc.get('metadata', {})
                })
        
        print(f"ğŸ¯ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        return results
    
    def generate_answer(self, question: str, documents: List[Dict]) -> str:
        """ç”Ÿæˆç­”æ¡ˆï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not documents:
            return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for doc in documents[:3]:  # åªä½¿ç”¨æœ€ç›¸å…³çš„3ä¸ªæ–‡æ¡£
            context_parts.append(f"ã€{doc['title']}ã€‘{doc['content']}")
        
        context = "\n\n".join(context_parts)
        
        # ç”Ÿæˆç­”æ¡ˆæ¨¡æ¿
        answer = f"æ ¹æ®ç›¸å…³æ–‡æ¡£ä¿¡æ¯å›ç­”æ‚¨çš„é—®é¢˜ï¼š\n\n{context}\n\nä»¥ä¸Šä¿¡æ¯æ¥è‡ªç›¸å…³æ–‡æ¡£ï¼Œå¸Œæœ›å¯¹æ‚¨æœ‰å¸®åŠ©ã€‚"
        
        return answer