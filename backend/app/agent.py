import json
from typing import AsyncGenerator, List
from .llm import LLMService
from .tools import ToolManager
from .vector_store import VectorStore
from .sandbox import PythonSandbox
from .db_guard import SQLGuard
from .prompt import PromptBuilder

class AgentEngine:
    def __init__(self):
        print("ğŸš€ [Agent] åˆå§‹åŒ–ï¼šKeep Logic & Enable Streaming...")
        self.llm = LLMService()
        self.tools = ToolManager()
        self.vector_store = VectorStore()
        self.sandbox = PythonSandbox()

    # --- ä¿æŒä½ çš„é€»è¾‘ä¸å˜ ---
    def _extract_previous_data(self, history: List[dict]):
        for msg in reversed(history):
            if msg.get("role") == "tool":
                try:
                    content = json.loads(msg.get("content", "{}"))
                    if content.get("status") == "success" and content.get("data"):
                        return content["data"]
                except: continue
        return None

    # --- ä¿æŒä½ çš„é€»è¾‘ä¸å˜ ---
    async def _execute_sql_with_retry(self, query: str):
        try:
            clean_sql = SQLGuard.validate(query)
        except ValueError as e:
            return {"status": "error", "message": str(e)}

        res = self.tools.execute("execute_sql", {"query": clean_sql})
        if res['status'] == 'success': return res

        # å¤±è´¥é‡è¯•é€»è¾‘ (å†…éƒ¨è°ƒç”¨ä¸ç”¨æµå¼ï¼Œä¿æŒ stream=False)
        error_msg = res['message']
        print(f"âš ï¸ [SQL Fail] {error_msg} -> Auto-fixing...")
        
        fix_prompt = f"SQL: {clean_sql}\nError: {error_msg}\nFix the SQL syntax. Output ONLY SQL."
        try:
            resp = await self.llm.chat_completion([
                {"role": "system", "content": "Output ONLY SQL. No markdown."},
                {"role": "user", "content": fix_prompt}
            ], temperature=0.0, stream=False) # å†…éƒ¨ä¿®å¤ä¸æµå¼
            
            fixed_sql = resp.choices[0].message.content.strip().replace("```sql", "").replace("```", "")
            clean_sql = SQLGuard.validate(fixed_sql)
            return self.tools.execute("execute_sql", {"query": clean_sql})
        except Exception as e:
            return {"status": "error", "message": f"Auto-fix failed: {e}"}

    async def run(self, history: List[dict]) -> AsyncGenerator[dict, None]:
        last_msg = history[-1]['content']
        intent = "DATA" if any(k in last_msg for k in ["æŸ¥", "åˆ†æ", "å›¾", "æ•°", "å¤šå°‘", "select", "æ’å"]) else "CHAT"

        # ----------------------------------------------------
        # åœºæ™¯ 1ï¼šé—²èŠæ¨¡å¼ (å¢åŠ æµå¼)
        # ----------------------------------------------------
        if intent == "CHAT":
            yield {"type": "trace", "data": {"status": "info", "message": "é—²èŠæ¨¡å¼"}}
            
            # ğŸ”¥ å¼€å¯æµå¼
            stream = await self.llm.chat_completion(history, temperature=0.7, stream=True)
            
            full_content = ""
            async for chunk in stream:
                if chunk.choices:
                    token = chunk.choices[0].delta.content
                    if token:
                        full_content += token
                        # ğŸ”¥ å®æ—¶åå­—
                        yield {"type": "text", "content": token}
            return

        # ----------------------------------------------------
        # åœºæ™¯ 2ï¼šæ•°æ®æ¨¡å¼ (RAG + å·¥å…· + æµå¼)
        # ----------------------------------------------------
        prev_data = self._extract_previous_data(history)
        context_data_buffer = prev_data if prev_data else []
        tools = self.tools.get_definitions()
        
        if context_data_buffer and any(k in last_msg for k in ["åˆ†æ", "ç”»", "å›¾", "è§£é‡Š"]):
            print("ğŸ§  [Mode] Analysis")
            prompt = PromptBuilder.build_analysis_prompt(json.dumps(context_data_buffer[:3], ensure_ascii=False), len(context_data_buffer))
            tools = [t for t in tools if t['function']['name'] != 'execute_sql']
        else:
            print("ğŸ§  [Mode] RAG Query")
            rag_results = self.vector_store.retrieve(last_msg, top_k=8)
            prompt = PromptBuilder.build_system_prompt(rag_results)

        msgs = [{"role": "system", "content": prompt}] + history[-5:]

        # 3è½®äº¤äº’ Loop
        for i in range(3):
            yield {"type": "trace", "data": {"status": "thinking", "message": "æ€è€ƒä¸­..."}}
            
            # ğŸ”¥ å¼€å¯æµå¼
            stream = await self.llm.chat_completion(msgs, tools=tools, temperature=0.0, stream=True)
            
            full_content = ""
            tool_calls_buffer = []
            current_tool_index = -1
            
            # ğŸ”¥ é€å—æ¥æ”¶å¹¶å¤„ç†
            async for chunk in stream:
                if not chunk.choices: continue
                delta = chunk.choices[0].delta

                # A. æ–‡æœ¬å†…å®¹ -> å®æ—¶å‘ç»™å‰ç«¯ (Thought)
                if delta.content:
                    token = delta.content
                    full_content += token
                    yield {"type": "thought", "content": token}

                # B. å·¥å…·è°ƒç”¨ -> å¿…é¡»æ‹¼æ¥ç¢ç‰‡ (ä¸èƒ½ç›´æ¥å‘)
                if delta.tool_calls:
                    for tool_delta in delta.tool_calls:
                        index = tool_delta.index
                        
                        # æ–°å·¥å…·å¼€å§‹
                        if index > current_tool_index:
                            tool_calls_buffer.append({
                                "id": tool_delta.id,
                                "type": "function",
                                "function": {
                                    "name": tool_delta.function.name,
                                    "arguments": ""
                                }
                            })
                            current_tool_index = index
                        
                        # æ‹¼æ¥å‚æ•°
                        if tool_delta.function.arguments:
                            tool_calls_buffer[index]["function"]["arguments"] += tool_delta.function.arguments

            # --- æµå¼æ¥æ”¶å®Œæ¯•ï¼Œå¼€å§‹æ‰§è¡Œé€»è¾‘ (å’ŒåŸæ¥ä¸€æ ·) ---
            
            if not tool_calls_buffer:
                if full_content:
                    yield {"type": "text", "content": full_content}
                break

            # å­˜å…¥å†å²
            msgs.append({
                "role": "assistant",
                "content": full_content,
                "tool_calls": tool_calls_buffer
            })

            for tool_call in tool_calls_buffer:
                func_name = tool_call["function"]["name"]
                args = self.tools.parse_args(tool_call["function"]["arguments"])
                
                yield {"type": "trace", "data": {"status": "executing", "tool": func_name}}
                
                tool_result = {}

                # 1. SQL
                if func_name == "execute_sql":
                    res = await self._execute_sql_with_retry(args.get("query"))
                    if res['status'] == 'success':
                        data = res['data']
                        context_data_buffer = data
                        summary = f"Query returned {len(data)} rows."
                        yield {"type": "table", "data": data[:50], "summary": summary}
                        tool_result = {"status": "success", "message": summary, "data": data}
                    else:
                        tool_result = {"status": "error", "message": res['message']}
                
                # 2. Chart (ä¸æ‰§è¡Œï¼Œç›´æ¥è¿”å›å‰ç«¯)
                elif func_name == "generate_chart":
                    if not context_data_buffer:
                        tool_result = {"status": "error", "message": "No data available."}
                    else:
                        yield {
                            "type": "chart",
                            "data": context_data_buffer,
                            "config": {
                                "type": args.get("chart_type", "bar"),
                                "xKey": args.get("x_key"),
                                "yKey": args.get("y_key"),
                                "title": args.get("title", "Chart")
                            }
                        }
                        tool_result = {"status": "success", "message": "Chart sent to frontend."}

                # 3. Python
                elif func_name == "execute_python":
                    if not context_data_buffer:
                        tool_result = {"status": "error", "message": "No data found."}
                    else:
                        py_res = self.sandbox.execute(args.get("code"), data_context=context_data_buffer)
                        if py_res['success']:
                            tool_result = {"status": "success", "output": py_res['stdout']}
                            yield {"type": "text", "content": f"```\n{py_res['stdout']}\n```"}
                            if py_res.get('chart_config'):
                                 yield {"type": "chart", "config": py_res['chart_config']}
                        else:
                            tool_result = {"status": "error", "message": py_res['error']}

                msgs.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "content": json.dumps(tool_result, ensure_ascii=False)
                })